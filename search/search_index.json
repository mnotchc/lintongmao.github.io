{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Here is a skim of this website: REGULARIZATION Dropout Method","title":"Home"},{"location":"#regularization","text":"Dropout Method","title":"REGULARIZATION"},{"location":"contact/","text":"Hey, visitor! \\alpha\\beta\\rho\\alpha\\chi\\alpha\\delta\\alpha\\beta\\rho\\alpha! Welcome to my website! I'm now a student @NEU(CN). You can reach me at lintongmao@outlook.com.","title":"Contact"},{"location":"dropout/","text":"1. What is dropout? Dropout is a simple and powerful regularization method used in modern neural networks. The key idea is to randomly drop units (along with their connections) from the neural network during training with a probability p . Figure 1 illustrates how dropout is applied to a standard neural network (when training). 2. NOTE: TEST-TIME DROPOUT The basic idea of this mechanism is very simple. But, do note that after a network with dropout is trained, i.e., when testing a network, the same dropout mechanism should not be applied to the network any more, for we want a robust network instead of a reduced one. we can't simply drop the dropout mechanism as well, since that will cause a significant difference in the outputs of the training model and the model being tested. A proper way is to use \"a single unthinned network that has smaller weights\", i.e., we multiply each of a layer's input (or equivalently, the outputs of the previous layer) with the probability p . Formally, for a network layer \\mathbf{y} = f(W\\mathbf{x}+\\mathbf{b}) , let \\mathbf{x}\\in \\mathbb{R}^d , the output after dropout is applied is given by d(\\mathbf{x}) = \\begin{cases} \\mathbf{m}\\odot \\mathbf{x}, &\\text{during training}\\\\ p\\mathbf{x}, &\\text{during testing} \\end{cases} \\mathbf{y} = f(Wd(\\mathbf{x})+\\mathbf{b}) where \\mathbf{m}\\in\\{0,1\\}^d is a dropout mask that is generated using a Bernoulli distribution of probability p . In modern deep learning tools like pytorch in which dropout is already integrated, such changes can be easily achieved by some simple \"switch\", for example, in pytorch, the two different ways of applying dropout can be done by model.train() and model.eval() . 3. Dropout is great. Dropout has proven to be a very effective regularization method, that is, it prevents neural networks, especially the \"deep\" ones, from overfitting. Moreover, it boosts the training process of a model. Srivastava et al. experimented this approach on many benchmark datasets and the results showed that the models applied with dropout not only trained faster but achieved less error rates than regular models. For more details, please refer to this paper: Srivastava et al. , 2014 . 3. Why is dropout effective? To be continued, soon. 4. When & How should dropout be used? Despite it being powerful in regularizing a deep neural model, the dropout method has its limits. Chiyuan Zhang et al., 2017 showed that dropout is not effective in CNN when the data are corrupted. As far as I know, dropout is, generally, applied to the feedforward connections in a model. p is normally set to 0.5 - 0.8. And, do not apply to a network dropout method and batch normalization simultaneously. This has been experimented and proved to be a bad thing to do, by Xiang Li et al., 2018 5. For further study, you can refer to: \u300a\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5b66\u4e60\u300b Srivastava et al. , Dropout: A simple way to prevent neural networks from overfitting.","title":"Dropout"},{"location":"dropout/#1-what-is-dropout","text":"Dropout is a simple and powerful regularization method used in modern neural networks. The key idea is to randomly drop units (along with their connections) from the neural network during training with a probability p . Figure 1 illustrates how dropout is applied to a standard neural network (when training).","title":"1. What is dropout?"},{"location":"dropout/#2-note-test-time-dropout","text":"The basic idea of this mechanism is very simple. But, do note that after a network with dropout is trained, i.e., when testing a network, the same dropout mechanism should not be applied to the network any more, for we want a robust network instead of a reduced one. we can't simply drop the dropout mechanism as well, since that will cause a significant difference in the outputs of the training model and the model being tested. A proper way is to use \"a single unthinned network that has smaller weights\", i.e., we multiply each of a layer's input (or equivalently, the outputs of the previous layer) with the probability p . Formally, for a network layer \\mathbf{y} = f(W\\mathbf{x}+\\mathbf{b}) , let \\mathbf{x}\\in \\mathbb{R}^d , the output after dropout is applied is given by d(\\mathbf{x}) = \\begin{cases} \\mathbf{m}\\odot \\mathbf{x}, &\\text{during training}\\\\ p\\mathbf{x}, &\\text{during testing} \\end{cases} \\mathbf{y} = f(Wd(\\mathbf{x})+\\mathbf{b}) where \\mathbf{m}\\in\\{0,1\\}^d is a dropout mask that is generated using a Bernoulli distribution of probability p . In modern deep learning tools like pytorch in which dropout is already integrated, such changes can be easily achieved by some simple \"switch\", for example, in pytorch, the two different ways of applying dropout can be done by model.train() and model.eval() .","title":"2. NOTE: TEST-TIME DROPOUT"},{"location":"dropout/#3-dropout-is-great","text":"Dropout has proven to be a very effective regularization method, that is, it prevents neural networks, especially the \"deep\" ones, from overfitting. Moreover, it boosts the training process of a model. Srivastava et al. experimented this approach on many benchmark datasets and the results showed that the models applied with dropout not only trained faster but achieved less error rates than regular models. For more details, please refer to this paper: Srivastava et al. , 2014 .","title":"3. Dropout is great."},{"location":"dropout/#3-why-is-dropout-effective","text":"To be continued, soon.","title":"3. Why is dropout effective?"},{"location":"dropout/#4-when-how-should-dropout-be-used","text":"Despite it being powerful in regularizing a deep neural model, the dropout method has its limits. Chiyuan Zhang et al., 2017 showed that dropout is not effective in CNN when the data are corrupted. As far as I know, dropout is, generally, applied to the feedforward connections in a model. p is normally set to 0.5 - 0.8. And, do not apply to a network dropout method and batch normalization simultaneously. This has been experimented and proved to be a bad thing to do, by Xiang Li et al., 2018","title":"4. When &amp; How should dropout be used?"},{"location":"dropout/#5-for-further-study-you-can-refer-to","text":"\u300a\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5b66\u4e60\u300b Srivastava et al. , Dropout: A simple way to prevent neural networks from overfitting.","title":"5. For further study, you can refer to:"}]}