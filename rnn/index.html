<!doctype html><html lang="en" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="author" content="lintongmao"><meta name="lang:clipboard.copy" content="Copy to clipboard"><meta name="lang:clipboard.copied" content="Copied to clipboard"><meta name="lang:search.language" content="en"><meta name="lang:search.pipeline.stopwords" content="True"><meta name="lang:search.pipeline.trimmer" content="True"><meta name="lang:search.result.none" content="No matching documents"><meta name="lang:search.result.one" content="1 matching document"><meta name="lang:search.result.other" content="# matching documents"><meta name="lang:search.tokenizer" content="[\s\-]+"><link rel="shortcut icon" href="../assets/images/favicon.png"><meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.3.1"><title>Notes on RNN - Abracadabra!</title><link rel="stylesheet" href="../assets/stylesheets/application.4031d38b.css"><script src="../assets/javascripts/modernizr.74668098.js"></script><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=swap"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel="stylesheet" href="../assets/fonts/material-icons.css"></head><body dir="ltr"><svg class="md-svg"><defs><svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg></defs></svg> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a href="#1-what-is-rnn" tabindex="1" class="md-skip">Skip to content </a><header class="md-header" data-md-component="header"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"><a href=".." title="Abracadabra!" class="md-header-nav__button md-logo"><i class="md-icon"></i></a></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--stretch"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">Abracadabra!</span><span class="md-header-nav__topic">Notes on RNN</span></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active"> <label class="md-icon md-search__icon" for="__search"></label> <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta">Type to start searching</div><ol class="md-search-result__list"></ol></div></div></div></div></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a href="https://github.com/lintongmao/lintongmao.github.io/" title="Go to repository" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">lintongmao/lintongmao.github.io</div></a></div></div></div></nav></header><div class="md-container"><main class="md-main"><div class="md-main__inner md-grid" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site" for="__drawer"><a href=".." title="Abracadabra!" class="md-nav__button md-logo"><i class="md-icon"></i></a>Abracadabra!</label><div class="md-nav__source"><a href="https://github.com/lintongmao/lintongmao.github.io/" title="Go to repository" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">lintongmao/lintongmao.github.io</div></a></div><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href=".." title="Home" class="md-nav__link">Home</a></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked><label class="md-nav__link" for="nav-2">Deep Learning Models</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-2">Deep Learning Models</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc"><label class="md-nav__link md-nav__link--active" for="__toc">Notes on RNN</label><a href="./" title="Notes on RNN" class="md-nav__link md-nav__link--active">Notes on RNN</a><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">Table of contents</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#1-what-is-rnn" title="1 What is RNN?" class="md-nav__link">1 What is RNN?</a></li><li class="md-nav__item"><a href="#2-variations-of-rnn" title="2 Variations of RNN" class="md-nav__link">2 Variations of RNN</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#21-vanilla-rnn-simple-recurrent-network-srn" title="2.1 Vanilla RNN / Simple Recurrent Network (SRN)" class="md-nav__link">2.1 Vanilla RNN / Simple Recurrent Network (SRN)</a></li><li class="md-nav__item"><a href="#22-long-short-term-memory-lstm" title="2.2 Long Short-Term Memory (LSTM)" class="md-nav__link">2.2 Long Short-Term Memory (LSTM)</a></li><li class="md-nav__item"><a href="#23-gated-recurrent-unit-gru" title="2.3 Gated Recurrent Unit (GRU)" class="md-nav__link">2.3 Gated Recurrent Unit (GRU)</a></li></ul></nav></li><li class="md-nav__item"><a href="#3-what-can-rnns-do" title="3 What can RNNs do?" class="md-nav__link">3 What can RNNs do?</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#31-it-can-handle-sequences" title="3.1 It can handle sequences." class="md-nav__link">3.1 It can handle sequences.</a></li><li class="md-nav__item"><a href="#32-it-excels-at-handling-non-sequential-data" title="3.2 It excels at handling non-sequential data" class="md-nav__link">3.2 It excels at handling non-sequential data</a></li></ul></nav></li><li class="md-nav__item"><a href="#4-conclusion" title="4 Conclusion" class="md-nav__link">4 Conclusion</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3"><label class="md-nav__link" for="nav-3">Regularization</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-3">Regularization</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../dropout/" title="Dropout" class="md-nav__link">Dropout</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4"><label class="md-nav__link" for="nav-4">About</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-4">About</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../contact/" title="Contact" class="md-nav__link">Contact</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">Table of contents</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#1-what-is-rnn" title="1 What is RNN?" class="md-nav__link">1 What is RNN?</a></li><li class="md-nav__item"><a href="#2-variations-of-rnn" title="2 Variations of RNN" class="md-nav__link">2 Variations of RNN</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#21-vanilla-rnn-simple-recurrent-network-srn" title="2.1 Vanilla RNN / Simple Recurrent Network (SRN)" class="md-nav__link">2.1 Vanilla RNN / Simple Recurrent Network (SRN)</a></li><li class="md-nav__item"><a href="#22-long-short-term-memory-lstm" title="2.2 Long Short-Term Memory (LSTM)" class="md-nav__link">2.2 Long Short-Term Memory (LSTM)</a></li><li class="md-nav__item"><a href="#23-gated-recurrent-unit-gru" title="2.3 Gated Recurrent Unit (GRU)" class="md-nav__link">2.3 Gated Recurrent Unit (GRU)</a></li></ul></nav></li><li class="md-nav__item"><a href="#3-what-can-rnns-do" title="3 What can RNNs do?" class="md-nav__link">3 What can RNNs do?</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#31-it-can-handle-sequences" title="3.1 It can handle sequences." class="md-nav__link">3.1 It can handle sequences.</a></li><li class="md-nav__item"><a href="#32-it-excels-at-handling-non-sequential-data" title="3.2 It excels at handling non-sequential data" class="md-nav__link">3.2 It excels at handling non-sequential data</a></li></ul></nav></li><li class="md-nav__item"><a href="#4-conclusion" title="4 Conclusion" class="md-nav__link">4 Conclusion</a></li></ul></nav></div></div></div><div class="md-content"><article class="md-content__inner md-typeset"><a href="https://github.com/lintongmao/lintongmao.github.io/edit/master/docs/rnn.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a><h1>Notes on RNN</h1><p>This article contains digests from:</p>
<ul>
<li><a href="https://nndl.github.io/">《神经网络与深度学习》</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
</ul>
<h3 id="1-what-is-rnn">1 What is RNN?</h3>
<p>Recurrent Neural Network is a class of neural networks with self-feedback neurons that are capable of handling sequences of variable-length. The figure below shows a typical RNN architecture.</p>
<p><img alt="a typical RNN architectire" src="../img/rnn.png" width=150 /></p>
<p>Given a sequence of inputs <script type="math/tex">\mathbf{x}_{1:T}=(\mathbf{x}_1, \mathbf{x}_2,\cdots,\mathbf{x}_T)</script>, the activation <script type="math/tex">\mathbf{h}_t</script> (namely, the hidden state at time <script type="math/tex">t</script>) is given by
<script type="math/tex; mode=display">
\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)
</script>
where <script type="math/tex">\mathbf{h}_0=\mathbf{0}</script> and <script type="math/tex">f(\cdot)</script> is a non-linearity (a non-linear function of a feed-forward net).</p>
<h3 id="2-variations-of-rnn">2 Variations of RNN</h3>
<h4 id="21-vanilla-rnn-simple-recurrent-network-srn">2.1 Vanilla RNN / Simple Recurrent Network (SRN)</h4>
<p>In a Simple Recurrent Network, <script type="math/tex">\mathbf{h}_t</script> is updated by
<script type="math/tex; mode=display">
\mathbf{h}_t = f(U\mathbf{h}_{t-1} + W\mathbf{x}_t + \mathbf{b})
</script>
where <script type="math/tex">U</script> is a <em>state-state</em> weight matrix,  <script type="math/tex">W</script> is a <em>state-input</em> weight matrix, and <script type="math/tex">\mathbf{b}</script> is a bias vector.</p>
<p>Theoretically, this model is capable of modeling any non-linear dynamical system. However, it suffers from the problems of <strong><em>gradient vanishing</em></strong> and <strong><em>gradient exploding</em></strong> (For details of how, refer to section 6.5 of  <a href="https://nndl.github.io/">this book</a> ) and thus can't really model dependency over a long range of a sequence, which is often called <strong><em>Long-Term Dependencies Problem</em></strong>.</p>
<h4 id="22-long-short-term-memory-lstm">2.2 Long Short-Term Memory (LSTM)</h4>
<p>Several solutions can be applied to avoid the problem above. To avoid gradient from exploding, we can use <u>weight decay</u> or <u>gradient clipping</u>. To avoid gradient from vanishing, apart from applying certain optimization techniques, we can modify the model further. A simple thought is to apply a mechanism very similar to Residual Networks,
<script type="math/tex; mode=display">
\mathbf{h}_t = \mathbf{h}_{t-1} + g(\mathbf{x}_t, \mathbf{h}_{t-1};\theta)
</script>
where <script type="math/tex">g(\cdot)</script> is a non-linearity (for details, again, refer to section 6.5 of  <a href="https://nndl.github.io/">this book</a> ).</p>
<p>Such a model still suffers from the gradient vanishing problem and the memory capacity problem, i.e., to some point <script type="math/tex">\mathbf{h}_t</script> can no longer store much informations.</p>
<p>Gated RNNs are an elegant modification of vanilla RNNs. One such example is LSTM, in which <script type="math/tex">\mathbf{h}_t</script> is updated by
<script type="math/tex; mode=display">
\begin{equation}\begin{split} 
\mathbf{c}_t &= \mathbf{f}_t\odot \mathbf{c}_{t-1} + \mathbf{i}_t\odot \tilde{\mathbf{c}}_t\\
\mathbf{h}_t &= \mathbf{o}_t\odot\tanh(\mathbf{c}_t)
\end{split}\end{equation}
</script>
where <script type="math/tex">\mathbf{c}_t</script> is the <em>internal state</em> for linear information passing (<script type="math/tex">\mathbf{c}</script> is short for <em>memory cell</em>), <script type="math/tex">{f}_t</script>, <script type="math/tex">\mathbf{i}_t</script> and <script type="math/tex">\mathbf{o}_t</script> are three gates used to control the flow of information, <script type="math/tex">\odot</script> is element-wise production, <script type="math/tex">\tilde{\mathbf{c}}_t</script> is the <em>candidate state</em>, which is given by
<script type="math/tex; mode=display">
\tilde{\mathbf{c}}_t = \tanh(U_c\mathbf{h}_{t-1} + W_c\mathbf{x}_t + \mathbf{b}_c)
</script>
</p>
<p>The three gates are used in the following manner:</p>
<ul>
<li><strong><em>forget</em></strong> gate <script type="math/tex">\mathbf{f}_t</script> controls how much information should be discarded from the previous internal state <script type="math/tex">\mathbf{c}_{t-1}</script>
</li>
<li><strong><em>input</em></strong> gate <script type="math/tex">\mathbf{i}_t</script> controls how much information should be stored from the candidate state <script type="math/tex">\tilde{\mathbf{c}}_t</script>
</li>
<li><strong><em>output</em></strong> gate <script type="math/tex">\mathbf{o}_t</script> controls how much information should be transferred from the internal state <script type="math/tex">\mathbf{c}_{t}</script> to external state <script type="math/tex">\mathbf{h}_{t}</script>
</li>
</ul>
<p>The values of the three gates are given by
<script type="math/tex; mode=display">
\begin{equation}\begin{split} 
\mathbf{f}_t &= \sigma(U_f\mathbf{h}_{t-1} + W_f\mathbf{x}_t + \mathbf{b}_f)\\
\mathbf{i}_t &= \sigma(U_i\mathbf{h}_{t-1} + W_i\mathbf{x}_t + \mathbf{b}_i)\\
\mathbf{o}_t &= \sigma(U_o\mathbf{h}_{t-1} + W_o\mathbf{x}_t + \mathbf{b}_o)
\end{split}\end{equation}
</script>
where <script type="math/tex">\sigma(\cdot)</script> is a Logistic function.</p>
<p>P.S.</p>
<p>In RNNs, <script type="math/tex">\mathbf{h}</script> stores information about history inputs. In SRNs, <script type="math/tex">\mathbf{h}</script> is rewritten at every time step <script type="math/tex">t</script>, thus can be perceived as a short-term memory. On the other hand, in LSTM, the <em>memory cell</em> <script type="math/tex">\mathbf{c}</script> stores history information for some time steps (controlled by forget gate), thus can be seen as a short-term memory that lasts longer than <script type="math/tex">\mathbf{h}</script>. This is how the name "Long Short-term Memory" comes about.</p>
<h4 id="23-gated-recurrent-unit-gru">2.3 Gated Recurrent Unit (GRU)</h4>
<p>Gated Recurrent Unit is another RNN that uses gating mechanism. However, it does not introduce an extra memory cell. In GRU, <script type="math/tex">\mathbf{h}_t</script> is updated by
<script type="math/tex; mode=display">
\mathbf{h}_t = \mathbf{z}_t\odot\mathbf{h}_{t-1} + (1-\mathbf{z}_t)\odot \tilde{\mathbf{h}}_t
</script>
where <script type="math/tex">\mathbf{z}_t\in [0,1]</script>, called <strong><em>update</em></strong> gate, is computed by
<script type="math/tex; mode=display">
\mathbf{z}_t = \sigma(U_z\mathbf{h}_{t-1} + W_z\mathbf{x}_t + \mathbf{b}_z)
</script>
and, <script type="math/tex">\tilde{\mathbf{h}}_t</script> is computed by
<script type="math/tex; mode=display">
\begin{equation}\begin{split}
\tilde{\mathbf{h}}_t &= \tanh(U_h(\mathbf{r}_t\odot\mathbf{h}_{t-1}) + W_h\mathbf{x}_t + \mathbf{b}_h)\\
\mathbf{r}_t &= \sigma(U_r\mathbf{h}_{t-1} + W_r\mathbf{x}_t + \mathbf{b}_r)
\end{split}\end{equation}
</script>
where <script type="math/tex">\mathbf{r}_t\in [0,1]</script>, called <strong><em>reset</em></strong> gate, controls how much the candidate state <script type="math/tex">\tilde{\mathbf{h}}_t</script> depends on the previous state <script type="math/tex">\mathbf{h}_{t-1}</script>.</p>
<p>Intuitively, the GRU uses only one gate, as opposed to two in LSTM, to control the balance between the operations of <em>forget</em> and <em>input</em>, making it a simpler network than LSTM.</p>
<h3 id="3-what-can-rnns-do">3 What can RNNs do?</h3>
<p>What exactly is a recurrent network capable of?</p>
<h4 id="31-it-can-handle-sequences">3.1 It can handle sequences.</h4>
<p>A glaring limitation of <strong>Vanilla</strong> Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). </p>
<blockquote>
<p>P.S. Vanilla means <em>"ordinary"</em> or <em>"plain"</em>, not a flavor of dessert. Here it means ordinary neural nets like BPNN, MLP; likewise, vanilla RNNs means ordinary RNNs.</p>
</blockquote>
<p><img alt="some situations for a deep learning system" src="../img/diags.jpeg" /></p>
<p>The above picture shows some situations for a deep learning system to model. The leftmost subfigure illustrates the situation where both the input and the output are fixed-sized. This can be done without RNNs.
The other subfigures show some more common situations, where a system has: <u>sequence output</u> (image captioning), <u>sequence input</u> (text sentiment classification), <u>asynced sequence input and output</u> (machine translation), and 
<u>synced sequence input and output</u> (POS tagging and NER).</p>
<p>RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, <strong><u>RNNs essentially describe programs</u></strong>. 
<strong><u>RNNs are Turing-Complete in the sense that they can to simulate arbitrary programs (with proper weights)</u></strong>. </p>
<p><strong>P.S.</strong></p>
<p>In <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.8383">Siegelmann and Sontag, 1991</a>, the authors models a recursive net with continuous-valued neurons as a dynamical system and remark that 
not only can one simulate a processor net with a Turing machine but any function computable by a Turing machine can be computed by a processor net. The paper shows a universal Turing machine can be simulated 
by a finite neural network made up of sigmoidal neurons.</p>
<blockquote>
<p>If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.</p>
</blockquote>
<h4 id="32-it-excels-at-handling-non-sequential-data">3.2 It excels at handling non-sequential data</h4>
<p>Even if the inputs/outputs are fixed vectors, it is still possible to use this powerful formalism to process them in a sequential manner.
The following picture illustrates a recurrent network that generates images of digits by learning to sequentially add color to a canvas (<a href="https://arxiv.org/abs/1502.04623">Gregor et al.</a>)
<img alt="house_generate.gif" src="../img/house_generate.gif" /></p>
<p>In doing this, you’re <u>learning stateful programs that process your fixed-sized data</u>, as opposed to merely learning a mapping between the input and the output.</p>
<h3 id="4-conclusion">4 Conclusion</h3>
<p>RNNs are powerful. For further study, please refer to the materials listed at the top of this page.</p></article></div></div></main><footer class="md-footer"><div class="md-footer-nav"><nav class="md-footer-nav__inner md-grid"><a href=".." title="Home" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev"><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-back md-footer-nav__button"></i></div><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">Previous</span>Home</span></div></a><a href="../dropout/" title="Dropout" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next"><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">Next</span>Dropout</span></div><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i></div></a></nav></div><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-footer-copyright">powered by <a href="https://www.mkdocs.org">MkDocs</a> and <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a></div></div></div></footer></div><script src="../assets/javascripts/application.b260a35d.js"></script><script>app.initialize({version:"1.0.4",url:{base:".."}})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>